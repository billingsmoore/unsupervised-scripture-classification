{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Buddhist Scriptures Using Unsupervised Machine Learning\n",
    "## Introduction\n",
    "### Problem Statement\n",
    "Buddhism is one of the world's major religions. Its primary branches are Theravada, Mahayana, and Vajrayana. These branches can be distinguished by, among many other features, which scriptures they accept as legitimate. Theravada is the most conservative in this respect, accepting the smallest set of scriptures. Mahayana accepts a larger set of scriptures. Vajrayana is the most liberal, accepting all of the scriptures of the Theravada, Mahayana, and an additional set unique to Vajrayana.\n",
    "Historians of religion, and religious practitioners, debate the historical origins of these branches and frequently when new texts are discovered by archaeologists, there is a question as to which branch these text belong. Machine learning methods can help us to categorize these texts in a way that may avoid both the possible sectarian divisions of practitioners and the possibly mistaken assumptions of historians.\n",
    "Thus, this project seeks to use unsupervised machine learning methods to categorize a group of texts as either Theravada, Mahayana, or Vajrayana.\n",
    "\n",
    "### Data\n",
    "I have collected excerpts from texts associated with each branch, saved as theravada-excerpts.txt, mahayana-excerpts.txt, and vajrayana-excerpts.txt. These excerpts are taken from scriptures that are taken by domain excerpts to be representative of the textual tradition of each branch. For each branch, the excerpts are taken from four separate texts in chunks of about 200 lines. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "First, I've loaded in the texts below and gotten a word count. We can see that there are a roughly equal number of words  (~5000) in each of the sets of excerpts. This is what we want for modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theravada Excerpts Word Count:5094\n",
      "Mahayana Excerpts Word Count:4885\n",
      "Vajrayana Excerpts Word Count:4863\n"
     ]
    }
   ],
   "source": [
    "with open('theravada-excerpts.txt', 'r') as excerpts:\n",
    "    theravada = excerpts.read().split(' ')\n",
    "\n",
    "print('Theravada Excerpts Word Count:' + str(len(theravada)))\n",
    "\n",
    "with open('mahayana-excerpts.txt', 'r') as excerpts:\n",
    "    mahayana = excerpts.read().split(' ')\n",
    "\n",
    "print('Mahayana Excerpts Word Count:' + str(len(mahayana)))\n",
    "\n",
    "with open('vajrayana-excerpts.txt', 'r') as excerpts:\n",
    "    vajrayana = excerpts.read().split(' ')\n",
    "\n",
    "print('Vajrayana Excerpts Word Count:' + str(len(vajrayana)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll split the sets of excerpts into chunks of 200 words to act as the samples in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "theravada_chunks = []\n",
    "mahayana_chunks = []\n",
    "vajrayana_chunks = []\n",
    "\n",
    "for i in range(0,len(theravada) - 199, 200):\n",
    "    theravada_chunks.append(' '.join(theravada[i:i+199]))\n",
    "\n",
    "for i in range(0,len(mahayana) - 199, 200):\n",
    "    mahayana_chunks.append(' '.join(mahayana[i:i+199]))\n",
    "\n",
    "for i in range(0,len(vajrayana) - 199, 200):\n",
    "    vajrayana_chunks.append(' '.join(vajrayana[i:i+199]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct our initial dataframe of text excerpts and category labels. I will use 0 for Theravada, 1 for Mahayana, and 2 for Vajrayana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THUS HAVE I HEARD. On one occasion the Blessed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>undisciplined in their\\n92 Sabbllsava Sutta: S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in him and the arisen\\ntaint ~f ignorance is a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>or the view 'I perceive not-\\nself with self' ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"What are the things unfit for attention that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  branch\n",
       "0  THUS HAVE I HEARD. On one occasion the Blessed...       0\n",
       "1  undisciplined in their\\n92 Sabbllsava Sutta: S...       0\n",
       "2  in him and the arisen\\ntaint ~f ignorance is a...       0\n",
       "3  or the view 'I perceive not-\\nself with self' ...       0\n",
       "4  \"What are the things unfit for attention that ...       0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_data = {'text':theravada_chunks, 'branch': [0]*(len(theravada_chunks))}\n",
    "th_df = pd.DataFrame(th_data)\n",
    "\n",
    "ma_data = {'text': mahayana_chunks, 'branch': [1]*(len(mahayana_chunks))}\n",
    "ma_df = pd.DataFrame(ma_data)\n",
    "\n",
    "va_data = {'text':vajrayana_chunks, 'branch': [2]*(len(vajrayana_chunks))}\n",
    "va_df = pd.DataFrame(va_data)\n",
    "\n",
    "df = pd.concat([th_df, ma_df, va_df])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying the Text\n",
    "Now that we have a dataframe of our text, we can start turning it into something quantifiable for modeling. I will use two approaches here. The first is the frequency analysis that we are already familiar with. The second is sentiment analysis.\n",
    "\n",
    "### Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean this up and tune the hyperparameters\n",
    "\n",
    "freq_matrix = TfidfVectorizer(sublinear_tf=True).fit_transform(df['text']).toarray()\n",
    "tsvd = TruncatedSVD(n_components=10).fit_transform(freq_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# this needs to be edited for purpose\n",
    "\n",
    "def assign(messages):\n",
    "    sentiments = []\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    for message in messages:\n",
    "        sentiment = sia.polarity_scores(message)\n",
    "        sentiments.append(sentiment)\n",
    "    return sentiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
